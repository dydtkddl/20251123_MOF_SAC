import numpy as np
from ase.neighborlist import neighbor_list
from ase.data import covalent_radii


class MOFEnv:

    # ============================================================
    def __init__(
        self,
        atoms_loader,
        k_neighbors=12,
        cmax=0.4,
        max_steps=300,
        fmax_threshold=0.05,
        bond_break_ratio=2.4,
        k_bond=3.0,
        max_penalty=10.0,
        debug_bond=False,
    ):
        self.atoms_loader = atoms_loader

        self.k = k_neighbors
        self.cmax = cmax
        self.max_steps = max_steps
        self.fmax_threshold = fmax_threshold

        # Bond-related
        self.bond_break_ratio = bond_break_ratio
        self.k_bond = k_bond
        self.max_penalty = max_penalty
        self.debug_bond = debug_bond

        # COM control (stabilized)
        self.com_threshold = 0.30
        self.com_lambda = 20.0    # ★ previously 100 × 0.1=10, now stabilized

        self.feature_dim = None
        self.reset()

    # ============================================================
    # True Bonds
    # ============================================================
    def _detect_true_bonds(self, atoms):

        i, j, offsets = neighbor_list("ijS", atoms, cutoff=4.0)

        pos = atoms.positions
        cell = atoms.cell

        bond_pairs = []
        bond_d0 = []

        for a, b, off in zip(i, j, offsets):
            rel = pos[b] + off @ cell - pos[a]
            d = np.linalg.norm(rel)

            rc = covalent_radii[atoms[a].number] + covalent_radii[atoms[b].number]

            if d <= rc + 0.4:
                bond_pairs.append((a, b))
                bond_d0.append(d)

        return np.array(bond_pairs, int), np.array(bond_d0, float)

    # ============================================================
    # Aromatic detection
    # ============================================================
    def _detect_aromatic_nodes(self, adj, Z):
        N = len(Z)
        aromatic = set()
        visited = set()

        def canonical(cycle):
            L = len(cycle)
            seqs = []
            for r in range(L):
                seqs.append(tuple(cycle[r:] + cycle[:r]))
            rev = list(reversed(cycle))
            for r in range(L):
                seqs.append(tuple(rev[r:] + rev[:r]))
            return min(seqs)

        def dfs(s, path, depth):
            if depth > 6:
                return
            last = path[-1]

            for nxt in adj[last]:
                if nxt == s and depth == 6:
                    cyc = canonical(path.copy())
                    if cyc not in visited:
                        if all(Z[x] == 6 and len(adj[x]) <= 3 for x in cyc):
                            aromatic.update(cyc)
                        visited.add(cyc)
                elif nxt > s and nxt not in path:
                    dfs(s, path + [nxt], depth + 1)

        for s in range(N):
            if Z[s] == 6 and len(adj[s]) <= 3:
                dfs(s, [s], 1)

        return aromatic

    # ============================================================
    def _assign_metal_flags(self, Z):
        MOF_METALS = {12,13,20,22,23,24,25,26,27,28,29,30,40,72}
        return np.array([1.0 if z in MOF_METALS else 0.0 for z in Z], float)

    def _detect_carboxylate_O(self, Z, adj, is_metal):
        N = len(Z)
        out = np.zeros(N, float)

        for O in range(N):
            if Z[O] != 8:
                continue
            for C in adj[O]:
                if Z[C] != 6:
                    continue

                O_list = [x for x in adj[C] if Z[x] == 8]
                if len(O_list) != 2:
                    continue

                if sum(is_metal[n] for n in adj[C]) >= 1:
                    out[O] = 1.0
                    break

        return out

    def _detect_mu_oxygens(self, Z, adj, is_metal):
        N = len(Z)
        mu2 = np.zeros(N, float)
        mu3 = np.zeros(N, float)

        for O in range(N):
            if Z[O] != 8:
                continue
            m = sum(is_metal[n] for n in adj[O])
            if m == 2:
                mu2[O] = 1.0
            elif m >= 3:
                mu3[O] = 1.0
        return mu2, mu3

    # ============================================================
    # RESET
    # ============================================================
    def reset(self):
        self.atoms = self.atoms_loader()
        self.N = len(self.atoms)

        self.forces = self.atoms.get_forces().astype(np.float32)
        self.prev_forces = np.zeros_like(self.forces)
        self.prev_disp = np.zeros_like(self.forces)

        Z = np.array([a.number for a in self.atoms])
        self.covalent_radii = np.array([covalent_radii[z] for z in Z]).astype(np.float32)

        self.bond_pairs, self.bond_d0 = self._detect_true_bonds(self.atoms)
        print(f"[INIT] Detected true bonds = {len(self.bond_pairs)}")

        self.adj = {i:[] for i in range(self.N)}
        for a,b in self.bond_pairs:
            self.adj[a].append(b)
            self.adj[b].append(a)

        aromatic_nodes = self._detect_aromatic_nodes(self.adj, Z)
        self.is_aromatic = np.zeros(self.N, float)
        self.is_aromatic[list(aromatic_nodes)] = 1.0

        self.is_metal = self._assign_metal_flags(Z)
        self.is_carboxylate_O = self._detect_carboxylate_O(Z, self.adj, self.is_metal)
        self.is_mu2O, self.is_mu3O = self._detect_mu_oxygens(Z, self.adj, self.is_metal)

        self.is_aromatic_C = np.zeros(self.N, float)
        for i in range(self.N):
            if Z[i] == 6 and self.is_aromatic[i] == 1.0:
                self.is_aromatic_C[i] = 1.0

        self.is_linker = np.zeros(self.N, float)
        for i in range(self.N):
            if (
                (not self.is_metal[i])
                and (not self.is_carboxylate_O[i])
                and (not self.is_aromatic_C[i])
                and Z[i] in [6,7]
            ):
                self.is_linker[i] = 1.0

        self.bond_types = np.zeros((self.N,6), float)

        for a,b in self.bond_pairs:
            Za, Zb = Z[a], Z[b]

            if self.is_metal[a] and Zb == 8: self.bond_types[a][0]+=1
            if self.is_metal[b] and Za == 8: self.bond_types[b][0]+=1

            if self.is_metal[a] and Zb == 7: self.bond_types[a][1]+=1
            if self.is_metal[b] and Za == 7: self.bond_types[b][1]+=1

            if self.is_carboxylate_O[a]: self.bond_types[b][2]+=1
            if self.is_carboxylate_O[b]: self.bond_types[a][2]+=1

            if self.is_aromatic_C[a] and self.is_aromatic_C[b]:
                self.bond_types[a][3]+=1
                self.bond_types[b][3]+=1

            if self.is_mu2O[a]: self.bond_types[b][4]+=1
            if self.is_mu2O[b]: self.bond_types[a][4]+=1

            if self.is_mu3O[a]: self.bond_types[b][5]+=1
            if self.is_mu3O[b]: self.bond_types[a][5]+=1

        self.feature_dim = len(self._make_feature(0))
        self.step_count = 0

        self.COM_prev = self.atoms.positions.mean(axis=0).astype(np.float32)

        return self._obs()

    # ============================================================
    def _rel_vec(self, i, j):
        disp = self.atoms.positions[j] - self.atoms.positions[i]
        cell = self.atoms.cell.array
        frac = np.linalg.solve(cell.T, disp)
        frac -= np.round(frac)
        return frac @ cell

    # ============================================================
    # Hops 1–3
    # ============================================================
    def _get_hop_sets(self, idx, max_hop=3):
        visited = set([idx])
        frontier = [idx]
        hop_map = {1:[], 2:[], 3:[]}

        for hop in range(1, max_hop+1):
            nxt_frontier=[]
            for node in frontier:
                for nxt in self.adj[node]:
                    if nxt not in visited:
                        visited.add(nxt)
                        nxt_frontier.append(nxt)
                        hop_map[hop].append(nxt)
            frontier = nxt_frontier

        return hop_map

    # ============================================================
    # Feature
    # ============================================================
    def _make_feature(self, idx):

        ri = self.covalent_radii[idx]
        gi = self.forces[idx]
        gprev = self.prev_forces[idx]

        gnorm = max(np.linalg.norm(gi), 1e-12)

        core = np.concatenate([
            np.array([ri, min(gnorm, self.cmax), np.log(gnorm+1e-6)]),
            gi,
            self.prev_disp[idx],
            gi - gprev,
        ])

        roles = np.array([
            self.is_aromatic[idx],
            self.is_metal[idx],
            self.is_linker[idx],
            self.is_carboxylate_O[idx],
            self.is_mu2O[idx],
            self.is_mu3O[idx],
        ])

        return np.concatenate([core, roles, self.bond_types[idx]])

    # ============================================================
    # Observation
    # ============================================================
    def _obs(self):

        obs_list = []

        for i in range(self.N):

            fi = self._make_feature(i)
            hop_sets = self._get_hop_sets(i)

            selected = []

            for j in hop_sets[1]:
                if len(selected) < self.k:
                    selected.append(j)

            for j in hop_sets[2]:
                if len(selected) < self.k:
                    selected.append(j)

            remain = self.k - len(selected)
            if remain>0 and len(hop_sets[3])>0:
                cand = hop_sets[3]
                if len(cand)<=remain:
                    selected += cand
                else:
                    selected += list(np.random.choice(cand, remain, False))

            while len(selected)<self.k:
                selected.append(None)

            nbr_feats=[]
            dists=[]
            vecs=[]

            for j in selected:
                if j is None:
                    nbr_feats.append(np.zeros_like(fi))
                    dists.append(0.0)
                    vecs.append(np.zeros(3))
                else:
                    fj = self._make_feature(j)
                    rel = self._rel_vec(i,j)
                    nbr_feats.append(fj)
                    dists.append(np.linalg.norm(rel))
                    vecs.append(rel)

            block = [fi]+nbr_feats
            block.append(np.array(dists))
            block.append(np.array(vecs).reshape(-1))

            obs_list.append(np.concatenate(block))

        return np.array(obs_list, float)

    # ============================================================
    # STEP  (Stabilized RL version)
    # ============================================================
    def step(self, action):

        self.step_count += 1
        action = np.clip(action, -1.0, 1.0)

        # -----------------------------
        # 1) Force-adaptive displacement
        # -----------------------------
        gnorm = np.linalg.norm(self.forces, axis=1)
        scale = np.minimum(gnorm, self.cmax).reshape(-1,1)

        disp = 0.003 * action * (scale / self.cmax)
        self.atoms.positions += disp

        new_forces = self.atoms.get_forces().astype(np.float32)

        old_norm = np.maximum(np.linalg.norm(self.forces,axis=1), 1e-12)
        new_norm = np.maximum(np.linalg.norm(new_forces,axis=1), 1e-12)

        # -----------------------------
        # 2) Force reward  (×10)
        # -----------------------------
        r_f = 10.0 * (np.log(old_norm+1e-6) - np.log(new_norm+1e-6))
        reward = r_f.copy()

        # -----------------------------
        # 3) COM penalty (stabilized)
        # -----------------------------
        COM_new = self.atoms.positions.mean(axis=0)
        delta_COM = np.linalg.norm(COM_new - self.COM_prev)

        reward -= self.com_lambda * delta_COM
        self.COM_prev = COM_new.copy()

        # done for COM drift (no penalty)
        if delta_COM > self.com_threshold:
            return self._obs(), reward, True

        # -----------------------------
        # 4) Bond penalty (×1, capped at 3)
        # -----------------------------
        for idx,(a,b) in enumerate(self.bond_pairs):

            rel = self._rel_vec(a,b)
            d = np.linalg.norm(rel)
            d0 = self.bond_d0[idx]
            ratio = d/d0

            stretch = max(0.0, ratio - self.bond_break_ratio)
            compress = max(0.0, 0.6 - ratio)

            penalty = 1.0 * self.k_bond * np.sqrt(stretch**2 + compress**2)
            penalty = min(penalty, 3.0)

            reward -= penalty

            # done if bond is broken (no reward penalty)
            if ratio>6.0 or ratio<0.25:
                return self._obs(), reward, True

        # -----------------------------
        # 5) Termination conditions
        # -----------------------------
        done = False
        if np.mean(new_norm) < self.fmax_threshold:
            done = True
        if self.step_count >= self.max_steps:
            done = True

        # -----------------------------
        # Update memory
        # -----------------------------
        self.prev_disp = disp.copy()
        self.prev_forces = self.forces.copy()
        self.forces = new_forces.copy()

        return self._obs(), reward, done
# sac/actor.py

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal


def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3):
    layer.weight.data.uniform_(-init_w, init_w)
    layer.bias.data.uniform_(-init_w, init_w)
    return layer


class Actor(nn.Module):
    """
    Per-Atom Actor Network
    obs_dim -> (mu, std) -> action(3,)
    """

    def __init__(
        self,
        obs_dim: int,
        act_dim: int = 3,     # ALWAYS 3 (dx,dy,dz)
        hidden_dim: int = 256,
        log_std_min: float = -10,
        log_std_max: float = 1.5,
    ):
        super().__init__()

        self.log_std_min = log_std_min
        self.log_std_max = log_std_max
        self.act_dim = act_dim

        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        self.mu_layer = nn.Linear(hidden_dim, act_dim)
        self.log_std_layer = nn.Linear(hidden_dim, act_dim)

        init_layer_uniform(self.mu_layer)
        init_layer_uniform(self.log_std_layer)

    def forward(self, obs):
        """
        obs: (batch, obs_dim)
        return: action(3,), log_prob, mu, std
        """

        # FP32 강화
        obs = obs.float()

        x = F.relu(self.fc1(obs))
        x = F.relu(self.fc2(x))

        mu = self.mu_layer(x)

        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
        std = torch.exp(log_std)

        # Gaussian reparameterization
        dist = Normal(mu, std)
        z = dist.rsample()

        # Output action in [-1,1]
        action = torch.tanh(z)

        # Tanh correction
        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(-1, keepdim=True)

        return action, log_prob, mu, std
# sac/agent.py

import numpy as np
import torch
import torch.nn.functional as F
import torch.optim as optim

from .actor import Actor
from .critic import CriticQ, CriticV


class SACAgent:
    """
    Stable per-atom SAC agent for MACS-MOF RL
    """

    def __init__(
        self,
        obs_dim,
        replay_buffer,
        act_dim=3,
        device="cuda",
        gamma=0.995,
        tau=5e-3,
        batch_size=256,
        lr=3e-4,
    ):

        self.replay = replay_buffer
        self.batch_size = batch_size

        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.gamma = gamma
        self.tau = tau

        # ---------------------------
        # NETWORKS (FP32)
        # ---------------------------
        self.actor = Actor(obs_dim, act_dim).to(self.device).float()
        self.v = CriticV(obs_dim).to(self.device).float()
        self.v_tgt = CriticV(obs_dim).to(self.device).float()
        self.q1 = CriticQ(obs_dim, act_dim).to(self.device).float()
        self.q2 = CriticQ(obs_dim, act_dim).to(self.device).float()

        self.v_tgt.load_state_dict(self.v.state_dict())

        # ---------------------------
        # OPTIMIZERS
        # ---------------------------
        self.actor_opt = optim.Adam(self.actor.parameters(), lr=lr)
        self.v_opt = optim.Adam(self.v.parameters(), lr=lr)
        self.q1_opt = optim.Adam(self.q1.parameters(), lr=lr)
        self.q2_opt = optim.Adam(self.q2.parameters(), lr=lr)

        # ---------------------------
        # ENTROPY (TARGET = -1)
        # ---------------------------
        self.target_entropy = -1.0
        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        self.alpha_opt = optim.Adam([self.log_alpha], lr=lr)

        self.total_steps = 0


    @property
    def alpha(self):
        return self.log_alpha.exp()


    # -------------------------------------------------------------
    # ACTION SELECTION
    # -------------------------------------------------------------
    @torch.no_grad()
    def act(self, obs):
        obs = torch.as_tensor(obs, dtype=torch.float32, device=self.device)
        a, _, _, _ = self.actor(obs)
        return a.cpu().numpy()


    # -------------------------------------------------------------
    # UPDATE SAC
    # -------------------------------------------------------------
    def update(self):

        # ------------------------
        # FIX: 항상 초기화
        # ------------------------
        policy_loss = None

        batch = self.replay.sample(self.batch_size)

        obs  = torch.as_tensor(batch["obs"],  dtype=torch.float32, device=self.device)
        act  = torch.as_tensor(batch["act"],  dtype=torch.float32, device=self.device)
        rew  = torch.as_tensor(batch["rew"],  dtype=torch.float32, device=self.device).unsqueeze(1)
        nobs = torch.as_tensor(batch["nobs"], dtype=torch.float32, device=self.device)
        done = torch.as_tensor(batch["done"], dtype=torch.float32, device=self.device).unsqueeze(1)

        # ===========================
        # α update
        # ===========================
        new_action, logp, _, _ = self.actor(obs)
        alpha_loss = -(self.log_alpha * (logp + self.target_entropy).detach()).mean()

        self.alpha_opt.zero_grad()
        alpha_loss.backward()
        self.alpha_opt.step()

        # ===========================
        # Q update
        # ===========================
        with torch.no_grad():
            v_next = self.v_tgt(nobs)
            q_target = rew + (1 - done) * self.gamma * v_next

        q1_pred = self.q1(obs, act)
        q2_pred = self.q2(obs, act)

        q1_loss = F.mse_loss(q1_pred, q_target)
        q2_loss = F.mse_loss(q2_pred, q_target)

        self.q1_opt.zero_grad()
        q1_loss.backward()
        self.q1_opt.step()

        self.q2_opt.zero_grad()
        q2_loss.backward()
        self.q2_opt.step()

        # ===========================
        # V update
        # ===========================
        v_pred = self.v(obs)

        with torch.no_grad():
            q_new = torch.min(
                self.q1(obs, new_action),
                self.q2(obs, new_action)
            )
        v_tgt = q_new - self.alpha * logp

        v_pred = v_pred.float()
        v_tgt  = v_tgt.float()

        v_loss = F.mse_loss(v_pred, v_tgt)

        self.v_opt.zero_grad()
        v_loss.backward()
        self.v_opt.step()


        # ===========================
        # Policy update every 2 steps
        # ===========================
        if self.total_steps % 2 == 0:

            aa, lp, _, _ = self.actor(obs)

            q_new2 = torch.min(
                self.q1(obs, aa),
                self.q2(obs, aa),
            )

            policy_loss = (self.alpha * lp - q_new2).mean()

            self.actor_opt.zero_grad()
            policy_loss.backward()
            self.actor_opt.step()

            self.soft_update()


        self.total_steps += 1

        # return losses (optional for logging)
        return {
            "policy_loss": float(policy_loss) if policy_loss is not None else None,
            "q1_loss": float(q1_loss),
            "q2_loss": float(q2_loss),
            "v_loss": float(v_loss),
            "alpha_loss": float(alpha_loss),
        }


    # -------------------------------------------------------------
    def soft_update(self):
        with torch.no_grad():
            for t, s in zip(self.v_tgt.parameters(), self.v.parameters()):
                t.data.copy_(self.tau * s.data + (1 - self.tau) * t.data)
# sac/critic.py

import torch
import torch.nn as nn
import torch.nn.functional as F


def init_layer_uniform(layer, w=3e-3):
    layer.weight.data.uniform_(-w, w)
    layer.bias.data.uniform_(-w, w)
    return layer


class CriticQ(nn.Module):
    """
    Per-Atom Q Network
    Q(obs_i, act_i)
    """

    def __init__(self, obs_dim: int, act_dim: int = 3, hidden_dim: int = 256):
        super().__init__()

        self.fc1 = nn.Linear(obs_dim + act_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, 1)

        init_layer_uniform(self.out)

    def forward(self, obs, act):
        # obs  (batch, obs_dim)
        # act  (batch, 3)
        x = torch.cat([obs.float(), act.float()], dim=-1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.out(x)


class CriticV(nn.Module):
    """
    Per-Atom V Network
    """

    def __init__(self, obs_dim: int, hidden_dim: int = 256):
        super().__init__()

        self.fc1 = nn.Linear(obs_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.out = nn.Linear(hidden_dim, 1)

        init_layer_uniform(self.out)

    def forward(self, obs):
        x = obs.float()
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.out(x)
# utils/replay_buffer.py

import numpy as np


class ReplayBuffer:
    """
    Stable MACS-style ReplayBuffer
    ---------------------------------------------------------
    Stores *per-atom transitions*:
        obs_i       : (obs_dim,)
        act_i       : (3,)          # dx, dy, dz
        reward_i    : float
        next_obs_i  : (obs_dim,)
        done        : bool
    ---------------------------------------------------------
    """

    def __init__(
        self,
        obs_dim: int,
        max_size: int = 5_000_000,
    ):
        """
        act_dim is fixed to 3 for stable MACS-style SAC.
        """

        self.obs_dim = obs_dim
        self.act_dim = 3                # ALWAYS dx, dy, dz
        self.max_size = max_size

        self.ptr = 0
        self.size = 0

        # Buffers
        self.obs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)
        self.nobs_buf = np.zeros((max_size, obs_dim), dtype=np.float32)

        self.act_buf = np.zeros((max_size, 3), dtype=np.float32)
        self.rew_buf = np.zeros(max_size, dtype=np.float32)
        self.done_buf = np.zeros(max_size, dtype=np.bool_)


    # ============================================================
    # STORE ONE ATOM TRANSITION
    # ============================================================
    def store(self, obs_i, act_i, rew_i, next_obs_i, done_i):
        """
        Parameters
        ----------
        obs_i : np.ndarray (obs_dim,)
        act_i : np.ndarray (3,)
        rew_i : float
        next_obs_i : np.ndarray (obs_dim,)
        done_i : bool
        """

        self.obs_buf[self.ptr] = obs_i
        self.act_buf[self.ptr] = act_i
        self.rew_buf[self.ptr] = rew_i
        self.nobs_buf[self.ptr] = next_obs_i
        self.done_buf[self.ptr] = done_i

        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)


    # ============================================================
    # SAMPLE MINI-BATCH
    # ============================================================
    def sample(self, batch_size):
        """
        Randomly sample per-atom transitions
        """

        idxs = np.random.randint(0, self.size, size=batch_size)

        return dict(
            obs=self.obs_buf[idxs],
            act=self.act_buf[idxs],
            rew=self.rew_buf[idxs],
            nobs=self.nobs_buf[idxs],
            done=self.done_buf[idxs],
        )


    def __len__(self):
        return self.size
##############################################
# train_mof_multi_env.py  
# Per-Atom RL version (MACS-style)
# Full Logging + XYZ Dump + Curriculum Horizon
##############################################

import os
import time
import numpy as np
import logging
from logging.handlers import RotatingFileHandler
from tqdm import tqdm
import torch

from ase.io import read
from mace.calculators import MACECalculator

from env.mof_env import MOFEnv
from sac.agent import SACAgent
from utils.replay_buffer import ReplayBuffer


##############################################
# LOGGING SETUP
##############################################
log_handler = RotatingFileHandler(
    "train.log",
    maxBytes=20_000_000,
    backupCount=10,
)
log_handler.setFormatter(logging.Formatter(
    "%(asctime)s - %(levelname)s - %(message)s"
))

logger = logging.getLogger("train")
logger.setLevel(logging.INFO)
logger.addHandler(log_handler)


##############################################
# CHECKPOINT
##############################################
def save_checkpoint(ep, agent, tag="auto"):
    os.makedirs("checkpoints", exist_ok=True)
    ckpt = {
        "epoch": ep,
        "actor": agent.actor.state_dict(),
        "q1": agent.q1.state_dict(),
        "q2": agent.q2.state_dict(),
        "v": agent.v.state_dict(),
        "v_tgt": agent.v_tgt.state_dict(),
        "log_alpha": float(agent.log_alpha.detach().cpu()),
    }
    p = f"checkpoints/ckpt_ep{ep:04d}_{tag}.pt"
    torch.save(ckpt, p)
    logger.info(f"[CHECKPOINT] Saved => {p}")


##############################################
# CIF SAMPLING
##############################################
POOL_DIR = "mofs/train_pool_valid"

def sample_cif():
    cifs = [
        os.path.join(POOL_DIR, f)
        for f in os.listdir(POOL_DIR)
        if f.endswith(".cif")
    ]
    return np.random.choice(cifs)


##############################################
# MACE Surrogate
##############################################
calc = MACECalculator(
    model_paths=["mofs_v2.model"],
    head="pbe_d3",
    device="cuda",
    default_dtype="float32"
)


##############################################
# CONFIG
##############################################
EPOCHS       = 1500
BASE_STEPS   = 300
FINAL_STEPS  = 1000
HORIZON_SCH  = 500

FMAX_THRESH  = 0.05
BUFFER_SIZE  = 5_000_000
BATCH_SIZE   = 256

CHECKPOINT_INTERVAL = 5


##############################################
# GLOBALS
##############################################
OBS_DIM = None
ACT_DIM = 3   # ALWAYS 3 for per-atom action
replay = None
agent = None


##############################################
# TRAIN START
##############################################
logger.info(f"[MACS-MOF] Training start (EPOCHS={EPOCHS})")
global_start = time.time()


for ep in range(EPOCHS):

    logger.info("\n" + "="*80)
    logger.info(f"[EP {ep}] START")

    ##################################
    # Curriculum Horizon
    ##################################
    ratio = min(ep / HORIZON_SCH, 1.0)
    max_steps = int(BASE_STEPS + (FINAL_STEPS - BASE_STEPS) * ratio)
    logger.info(f"[EP {ep}] max_steps = {max_steps}")

    ##################################
    # Snapshot folders
    ##################################
    snap_dir = f"snapshots/EP{ep:04d}"
    os.makedirs(snap_dir, exist_ok=True)

    traj_path = os.path.join(snap_dir, "traj.xyz")
    en_path = os.path.join(snap_dir, "energies.txt")

    if os.path.exists(traj_path): os.remove(traj_path)
    if os.path.exists(en_path): os.remove(en_path)

    ##################################
    # Load CIF and Init Env
    ##################################
    cif = sample_cif()
    atoms = read(cif)
    atoms.calc = calc

    env = MOFEnv(
        atoms_loader=lambda: atoms,
        k_neighbors=12,
        fmax_threshold=FMAX_THRESH,
        max_steps=max_steps,
        cmax=0.03,
    )

    obs = env.reset()
    logger.info(f"[EP {ep}] CIF loaded: {cif}")

    N_atom = env.N
    obs_dim = obs.shape[1]   # per-atom feature dim

    ##################################
    # EP0: Initialize Replay + Agent
    ##################################
    if ep == 0:
        OBS_DIM = obs_dim
        logger.info(f"[INIT] OBS_DIM={OBS_DIM}, ACT_DIM=3 (per-atom)")

        replay = ReplayBuffer(
            obs_dim=OBS_DIM,
            max_size=BUFFER_SIZE
        )

        agent = SACAgent(
            obs_dim=OBS_DIM,
            act_dim=3,
            replay_buffer=replay,
            device="cuda",
            lr=3e-4,
            gamma=0.995,
            tau=5e-3,
            batch_size=BATCH_SIZE,
        )
        logger.info("[INIT] Agent + ReplayBuffer allocated (per-atom).")


    ##################################
    # EPISODE
    ##################################
    ep_ret = 0.0

    for step in tqdm(range(max_steps), desc=f"[EP {ep}]", ncols=120):

        ########################
        # ACTION (per-atom)
        ########################
        obs_tensor = obs  # shape = (N_atom, obs_dim)

        action_list = []
        for i in range(N_atom):
            a = agent.act(obs_tensor[i])  # → (3,)
            action_list.append(a)

        action_arr = np.stack(action_list, axis=0)  # (N_atom, 3)

        ########################
        # STEP ENV
        ########################
        next_obs, reward, done = env.step(action_arr)
        # reward = per-atom reward shape (N_atom,)

        ########################
        # STORE (per-atom)
        ########################
        next_reward = reward.astype(np.float32)

        for i in range(N_atom):
            replay.store(
                obs[i],            # (obs_dim,)
                action_arr[i],     # (3,)
                next_reward[i],    # scalar
                next_obs[i],       # (obs_dim,)
                done,
            )

        if len(replay) > agent.batch_size:
            agent.update()

        ########################
        # LOG & SAVE TRAJECTORY
        ########################
        env.atoms.write(traj_path, append=True)

        Etot = env.atoms.get_potential_energy()
        E_pa = Etot / N_atom

        with open(en_path, "a") as f:
            f.write(f"{step} {Etot:.8f} {E_pa:.8f}\n")

        f_norm = np.linalg.norm(env.forces, axis=1)

        logger.info(
            f"[EP {ep}][STEP {step}] "
            f"N={N_atom} | "
            f"Favg={np.mean(f_norm):.6f} Fmax={np.max(f_norm):.6f} "
            f"rew_mean={float(np.mean(next_reward)):.6f} | "
            f"replay={len(replay):,} | "
            f"alpha={float(agent.alpha):.5f}"
        )

        ep_ret += float(np.mean(next_reward))
        obs = next_obs

        if done:
            logger.info(f"[EP {ep}] terminated early at step={step}")
            break


    ##################################
    # EP END
    ##################################
    logger.info(f"[EP {ep}] return={ep_ret:.6f}")
    logger.info(f"[EP {ep}] replay_size={len(replay):,}")

    if ep % CHECKPOINT_INTERVAL == 0 and ep > 0:
        save_checkpoint(ep, agent, tag="interval")


##############################################
# FINAL SAVE
##############################################
save_checkpoint(EPOCHS, agent, tag="final")

logger.info("[TRAIN DONE]")
logger.info(f"wallclock={(time.time() - global_start)/3600:.3f} hr")

print("== training finished ==")
