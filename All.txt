| 파일명                       | 변경 위치 / 대상                                         | 변경 필요 이유                                                                     | 구체적 수정 사항 / 힌트                                                                                                                                                                                                                                   |
| ------------------------- | -------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `main_train.py`           | `env.step(...)` 호출부 및 반환값 언패킹                      | `MOFEnv.step()` 반환값이 7개 → 5개(`obs_atom, obs_global, reward, done, info`)로 바뀜 | 예: `obs_atom, obs_global, reward, done, reason, Etot, Fmax = env.step(actions)` → `obs_atom, obs_global, reward, done, info = env.step(actions)` 로 변경. 이후 `reason, Etot, Fmax` 사용 부분은 `info["done_reason"]`, `info["Etot"]`, `info["Fmax"]`로 대체. |
| `main_train.py`           | 에피소드 종료 로깅 / 저장 로직 (done_reason 등)                 | 이전에는 로컬 변수 `reason`, `Fmax` 등을 직접 사용                                         | `if done:` 블록 안에서 `reason` 대신 `info.get("done_reason")`, `Fmax` 대신 `info.get("Fmax")` 사용. CIF 경로 필요 시 `env.current_cif_path`도 함께 로깅/저장하도록 추가.                                                                                                    |
| `sac/agent.py`            | `replay_buffer.sample()` 호출부                       | `sample()`이 이제 `(batch, idxs, weights)`를 반환하고, 내부 키가 `done`으로 변경됨            | 예: `batch = replay_buffer.sample()` → `batch, idxs, weights = replay_buffer.sample()`. 이후 코드에서 `batch["dones"]`를 참조하면 오류이므로 모두 `batch["done"]`로 변경. 이미 `batch["weights"]`를 쓰고 있다면 그대로 사용 가능.                                                     |
| `sac/agent.py`            | PER 업데이트 (`update_priority` / `update_priorities`) | alias 추가로 둘 다 동작하지만, 코드 정합성 차원에서 확인 필요                                       | 현재 코드가 `replay_buffer.update_priority(idxs, td_errors)` 또는 `update_priorities(...)` 중 무엇을 쓰는지 확인. 둘 다 동작하지만 한 쪽으로 통일하고 싶으면 `update_priority(...)` 호출로 맞추는 것을 추천.                                                                                 |
| `sac/agent.py`            | done 텐서 이름 (`dones_t` vs `done_t`) 처리              | `sample()`에서 torch 텐서 키가 `done_t`로 바뀜                                        | 이후 네트워크 업데이트 시 `batch["dones_t"]`를 참조하는 코드가 있다면 `batch["done_t"]`로 변경. NumPy 버전도 마찬가지로 `batch["dones"]` → `batch["done"]`로 통일.                                                                                                                   |
| (옵션) `main_train.py` / 기타 | `replay_buffer.store(...)` 호출부                     | 새 래퍼 `store(...)` 추가로, 기존 `store_step(...)` 대신 더 일반적인 인터페이스 사용 가능            | 현재 `store_step(obs_atom, actions, reward, next_obs_atom, done)`를 직접 호출 중이라면 그대로 둬도 무방. SAC 스타일로 맞추고 싶으면 `replay_buffer.store(obs, acts, rews, next_obs, done)` 형태로 교체하고, 나머지 인자(atom_type 등)는 생략해도 됨(무시됨).                                       |

스텝바이 스텝 기존 로직 생각해서 내가 명세한거 전부 들어가도록 코드가 길어도되니까 논리적으로 정합성맞게 전체 코드부탁해